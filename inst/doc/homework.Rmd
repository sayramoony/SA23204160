---
title: "Homework"
author: "Liangchen He"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Homework 0 (2023-09-11)

### Example 1
```{r}
lmR <- function(y,X){
  if(length(y)!=nrow(X)){
    print("The matrix is not inversable！")
    return()
  }
  else return(solve(t(X)%*%X)%*%t(X)%*%y)
}
```

### Example 2
|  C1 |   C2  |C3 |
|:----|:-----:|--:|
|  1  | left  | a |
|  2  | center| b |
|  3  | right | c |

### Example 3
```{r}
x <- 1:5
y <- c(4,6,9,2,5)
plot(x,y)
```

## Homework 1 (2023-09-18)

### 3.2

#### Question
The standard Laplace distribution has density $f(x) = \frac{1}{2}e^{-|x|}, x \in \mathbb{R}$. Use the inverse transform method to generate a random sample of size 1000 from this distribution. Use one of the methods shown in this chapter to compare the generated sample to the target distribution.

#### Answer
The cumulative distribution function is
$$
F(x)=
\begin{cases}
\frac{1}{2}e^x,\quad x\le 0,\\
1-\frac{1}{2}e^{-x},\quad x> 0,
\end{cases}
$$
thus
$$
F^{-1}(u)=
\begin{cases}
\log(2u),\quad 0\le u \le \frac{1}{2},\\
\log(2(1-u)), \quad \frac{1}{2}<u\le 1.
\end{cases}
$$

```{r}
set.seed(123)
n <- 1000
u <- runif(n)
x <- ifelse(u<=1/2, log(2*u), -log(2*(1-u)))
hist(x, prob = TRUE, xlim = c(-6,6), ylim = c(0,0.5))
#lines(density(x))
y <- seq(-6, 6, 0.01)
lines(y, 1/2*exp(-abs(y)))
```

### 3.7
#### Question
Write a function to generate a random sample of size n from the Beta(a, b)
distribution by the acceptance-rejection method. Generate a random sample
of size 1000 from the Beta(3,2) distribution. Graph the histogram of the
sample with the theoretical Beta(3,2) density superimposed.

#### Answer
The probability density function of Beta(a, b) is
$$
f(x;a,b)=\frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1}.
$$

```{r}
mybeta <- function(a, b, n){
  j<-k<-0
  y <- numeric(n)
  while (k < n) {
    u <- runif(1)
    j <- j + 1
    x <- runif(1) #random variate from g(.)
    if (x^(a-1) * (1-x)^(b-1) > u) {
      #we accept x
      k <- k + 1
      y[k] <- x
    }
  }
  return(y)
}
```

```{r}
a <- 3
b <- 2
n <- 1000
set.seed(123)
x <- mybeta(a, b, n)
hist(x, prob = TRUE, xlim = c(0,1), ylim = c(0,2))
y <- seq(0, 1, 0.01)
lines(y, 1/beta(a, b)*y^(a-1)*(1-y)^(b-1))
```

### 3.9
#### Question
The rescaled Epanechnikov kernel [85] is a symmetric density function
$$
f_e(x)=\frac{3}{4}(1-x^2),\quad |x|\le 1.
$$
Devroye and Györfi [71, p. 236] give the following algorithm for simulation from this distribution. Generate iid $U_1, U_2, U_3 \sim \text{Uniform}(−1, 1)$. If $|U_3| \ge |U_2|$ and $|U_3| \ge |U_1|$, deliver $U_2$; otherwise deliver $U_3$. Write a function to generate random variates from $f_e$, and construct the histogram density estimate of a large simulated random sample.

#### Answer
```{r}
myEpanechnikov <- function(n){
  x <- numeric(n)
  for(i in 1:n){
      u <- runif(3, min = -1, max = 1)
    if(abs(u[3])>=abs(u[2]) & abs(u[3])>=abs(u[1])) x[i] <- u[2]
    else x[i] <- u[3]
  }
return(x)
}
```

```{r}
n <- 1000
#set.seed(123)
x <- myEpanechnikov(n)
hist(x, prob = TRUE, xlim = c(-1,1), ylim = c(0,1))
lines(density(x))
```

### 3.10

#### Question
Prove that the algorithm given in Exercise 3.9 generates variates from the
density $f_e$.

#### Answer
Let $X$ be a random variable which probability distribution function is $f_e(x)$. Then according to the algorithm we have
\begin{align}
P(X\le x)&=P(U_2\le x, |U_3| \ge |U_2|, |U_3| \ge |U_1|)+P(U_3\le x)-P(U_3\le x, |U_3| \ge |U_2|, |U_3| \ge |U_1|)\\
&=\int_{-1}^{x}\frac{1}{2}P(|U_3| \ge |t|, |U_3| \ge |U_1|)\text{d}t+\frac{x+1}{2}-\int_{-1}^{x}\frac{1}{2}P(|U_2|\le |t|, |U_1|\le |t|)\text{d}t\\
&=\int_{-1}^{x}\frac{1}{2}\left[(\int_{-1}^{-|t|}+\int_{|t|}^{1})\frac{1}{2}P(|U_1|\le |s|)\text{d}s\right]\text{d}t+ \frac{x+1}{2}-\int_{-1}^{x}\frac{1}{2}t^2\text{d}t\\
&=\int_{-1}^{x}\frac{1}{4}(1-t^2)\text{d}t+\frac{x+1}{2}-\frac{1}{6}(x^3+1)\\
&=\frac{3}{4}x-\frac{1}{4}x^3+\frac{1}{2}.
\end{align}
In the above derivation we use the law of total probability. It is easy to find that $\frac{\text{d}P(X\le x)}{\text{d}x}=f_e(x)$, then we finish the proof.

### Extra Exercise
#### Question
Use the inverse transform method to reproduce some usages of `sample`, with the argument `replace = TRUE`.

#### Answer
```{r eval=FALSE, include=FALSE}
## The acceptance-rejection method can not be applied to this problem, mainly because we generate a continuous random variable X=x from g and then we check whether f(x)/cg(x)>U. If so, we accept this x. Here since the objective random variable is discrete, we can not map a continuous one to a discrete one one by one. Thus even we get X, we can not get X from f.

mysample <- function (vec, size, prob = NULL) {
  l <- length(vec)
  if (is.null(prob)) prob <- rep(1/l, l)
  sort.vec <- sort(vec)
  sort.prob <- prob[order(vec)]
  j <- k <- 0
  y <- numeric(size)
  while (k < size) {
    u <- runif(1)
    j <- j + 1
    x <- runif(1) #random variate from g(.)
    temp <- which(sort.vec >= x)
    if (length(temp) != 0) {
      temp.prob <- sort.prob[length(temp)] 
      #find the probability of the smallest value no less than x
      if (temp.prob > u) {
        #we accept x
        k <- k + 1
        y[k] <- sort.vec[length(temp)]
      }
    }
    # if length(temp) == 0, we drop this and do another, because we can not map x to a value from f. The map is on the left of each point value.
  }
  return(y)
}
```

```{r}
mysample <- function (x, size, prob = rep(1/length(x), length(x))) {
  sort.x <- sort(x)
  sort.prob <- prob[order(x)]
  cumprob <- cumsum(sort.prob)
  u <- runif(size)
  r <- findInterval(u, cumprob) + 1 # the index of the interval is from 0 to length(x), each interval is on the left of each point in x and concluding x.
  return(sort.x[r])
}

mysample(1:10, 3)

```

## Homework 3 (2023-09-25)

### Extra Exercise
#### Question
- Find the value of $\rho=\frac{l}{d} (0 \le ρ \le 1)$ that minimizes the asymptotic variance of $\hat{\pi}$.
- Take three different values of $\rho (0 \le ρ \le 1)$, including $\rho_{\min}$. Use Monte Carlo simulation to verify your answer and set $n = 10^6$, number of repeated simulations $K = 100$.

#### Answer
- It is easy to find that $m\sim B(n,\frac{2l}{d\pi})$. Denote $\rho=\frac{l}{d}$, $T_n=\frac{m}{n}$ and $\theta=\frac{2}{\pi}\rho$. 
According to CLT, we have
\begin{equation*}
\sqrt{n}(\frac{m}{n}-\frac{2}{\pi}\rho)\stackrel{\mathrm{d}}{\to}\mathcal{N}\left(0,\frac{2}{\pi}\rho(1-\frac{2}{\pi}\rho)\right),
\end{equation*}
or
\begin{equation*}
\sqrt{n}(T_{n}-\theta)\stackrel{\mathrm{d}}{\to}\mathcal{N}\left(0,\theta(1-\theta)\right),
\end{equation*}
Since $\hat{\pi}=\frac{2\rho}{T_n}$, due to the Delta method, we can deduce that
\begin{equation*}
\sqrt{n}(\hat{\pi}-\frac{2\rho}{\theta})\stackrel{\mathrm{d}}{\to}\mathcal{N}\left(0,\frac{4\rho^2}{\theta^4}\theta(1-\theta)\right),
\end{equation*}
then the asymptotic variance of $\hat{\pi}$ is $\frac{1}{n}\frac{4\rho^2}{\theta^4}\theta(1-\theta)=\frac{1}{n}(\frac{\pi^3}{2\rho}-\pi^2)$. Since $\rho\in(0,1]$, we get the minimum when $\rho=1$.

- We choose $\rho=0.1,0.5,1$.
```{r}
set.seed(12345)
l <- c(0.1,0.5,1)
d <- 1
n <- 1e6
K <- 100
pi_hat <- numeric(0)
for(k in 1:K) {
  X <- runif(n,0,d/2)
  Y <- runif(n,0,pi/2)
  pihat <- numeric(3)
  for(i in 1:3) {
    pihat[i] <- 2*l[i]/d/mean(l[i]/2*sin(Y)>X)
  }
  pi_hat <- rbind(pi_hat, pihat)
}

# the value of pi_hat
apply(pi_hat, 2, mean)

# asymptotic variance of pi_hat
apply(pi_hat, 2, var)/(K-1)
```

From the results above we can see that when $\rho=1$ the variance is the smallest.
(However the estimate and the theoretical variance are not in the same order of magnitude, maybe I construct the wrong estimate.)

### 5.6
#### Question
In Example 5.7 the control variate approach was illustrated for Monte Carlo integration of
\begin{equation*}
\theta=\int_{0}^{1} e^{x} d x
\end{equation*}
Now consider the antithetic variate approach. Compute $\operatorname{Cov}\left(e^{U}, e^{1-U}\right)$ and $\operatorname{Var}\left(e^{U}+e^{1-U}\right)$, where $U \sim \operatorname{Uniform}(0,1)$. What is the percent reduction in variance of $\hat{\theta}$ that can be achieved using antithetic variates (compared with simple MC)?

#### Answer
\begin{align*}
\operatorname{Cov}\left(e^{U}, e^{1-U}\right)&=\operatorname{E}\left[(e^{U}-\operatorname{E}e^{U}) (e^{1-U}-\operatorname{E}e^{1-U})\right]\\
&=\operatorname{E}\left[(e^{U}-(e-1)) (e^{1-U}-(e-1))\right]\\
&=e-(e-1)^2=-e^2+3e-1\approx -0.2342106.
\end{align*}

\begin{align*}
\operatorname{Var}\left(e^{U}+e^{1-U}\right)&=\operatorname{Var}(e^{U})+\operatorname{Var}(e^{1-U})+2\operatorname{Cov}\left(e^{U}, e^{1-U}\right)\\
&=\operatorname{E}e^{2U}-(\operatorname{E}e^{U})^2+\operatorname{E}e^{2-2U}-(\operatorname{E}e^{1-U})^2+2(e-(e-1)^2)\\
&=\frac{1}{2}(e^2-1)-(e-1)^2+\frac{1}{2}(e^2-1)-(e-1)^2+2(e-(e-1)^2)\\
&=-3e^2+10e-5\approx 0.01564999.
\end{align*}

The simple Monte Carlo estimator $\hat{\theta}_s=\frac{1}{m}\sum_{j=1}^m e^{U_j}$, thus $\operatorname{Var}(\hat{\theta}_s)=\frac{1}{m}\operatorname{Var}(e^U)=\frac{1}{m}(\frac{1}{2}(e^2-1)-(e-1)^2)=\frac{-e^2+4e-3}{2m}$. The antithetic variable $\hat{\theta}_a=\frac{1}{m}\sum_{j=1}^{m/2}(e^{U_j}+e^{1-U_j})$, thus $\operatorname{Var}(\hat{\theta}_a)=\frac{1}{2m}\operatorname{Var}\left(e^{U}+e^{1-U}\right)=\frac{-3e^2+10e-5}{2m}$. Then the percent reduction in variance is $\frac{\operatorname{Var}(\hat{\theta}_s)-\operatorname{Var}(\hat{\theta}_a)}{\operatorname{Var}(\hat{\theta}_s)}=\frac{2e^2-6e+2}{-e^2+4e-3}\approx 0.9676701$.

### 5.7
#### Question
Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate $\theta$ by the antithetic variate approach and by the simple Monte Carlo method. Compute
an empirical estimate of the percent reduction in variance using the antithetic variate. Compare the result with the theoretical value from Exercise 5.6.

#### Answer
```{r}
# the simple Monte Carlo method
m <- 1e6
set.seed(123)
u <- runif(m)
mc_s <- exp(u)
thetahat_s <- mean(mc_s)
v_s <- var(mc_s)/(m-1)

# the antithetic variate approach
m <- 1e6
set.seed(123)
u <- runif(m/2)
mc_a <- (exp(u)+exp(1-u))/2
thetahat_a <- mean(mc_a)
v_a <- var(mc_a)/(m/2-1)

# an empirical estimate of the percent reduction in variance using the antithetic variate
(v_s-v_a)/v_s
```
We can see that the empirical estimate 0.9676384 is close to the theoretical value 0.9676701.

## Homework 4 (2023-10-09)

### Extra exercise
#### Question
Prove that if $g$ is a continuous function on $(a_0,a_k)$, then $Var(\hat{\theta}^{S})/Var(\hat{\theta}^{M})\to 0$ as $a_i-a_{i-1}\to 0$ for all $i=1,\dots,k$.

#### Answer
Since $U=\sum_{i=1}^{k}UI(U\in I_i)$, we have
\begin{align*}
Var(\hat{\theta}^M)&=\frac{1}{M}Var(g(U))=\frac{1}{M}Var[\sum_{i=1}^{k} g(U)I(U\in I_i)] \\
&=\frac{1}{M}Var[E[g(U)|I(U\in J)]]+\frac{1}{M}E[Var[g(U)|I(U\in J)]]\\
&=\frac{1}{M}Var(\theta_J)+\frac{1}{M}\frac{1}{k}\sum_{i=1}^{k}\sigma_j^2=\frac{1}{M}Var(\theta_J)+\frac{1}{M}Var(\hat{\theta}^S),
\end{align*}
where J is a random interval. Thus
\begin{align*}
\frac{Var(\hat{\theta}^S)}{Var(\hat{\theta}^M)}=M-\frac{Var(\theta_J)}{Var(\hat{\theta}^M)}\to 0, a_i-a_{i-1}\to 0.
\end{align*}


### 5.13 & 5.14
#### Question
Find two importance functions $f_1$ and $f_2$ that are supported on $(1,\infty)$ and are ‘close’ to 
\begin{align*}
g(x)=\frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2}, \quad x>1.
\end{align*}
Which of your two importance functions should produce the smaller variance in estimating
\begin{align*}
\int_{1}^{\infty} \frac{x^{2}}{\sqrt{2 \pi}} e^{-x^{2} / 2} \text{d} x
\end{align*}
by importance sampling? Explain.

#### Answer
Notice that all the functions mentioned next are supported on $(1,\infty)$. We choose $f_1(x)=\frac{\sqrt{2\pi e}}{x}$ and $f_2(x)=\frac{1}{(1-\Phi(1))x^2}$. Then let
\begin{align*}
\phi_1(x)=g(x)f_1(x)=\sqrt{e}xe^{-\frac{x^2}{2}}\text{d}x
\end{align*}
and
\begin{align*}
\phi_2(x)=g(x)f_2(x)=\frac{1}{\sqrt{2\pi}(1-\Phi(1))}e^{-\frac{x^2}{2}}\text{d}x,
\end{align*}
we have
\begin{align*}
\int_1^{\infty}g(x)\text{d}x=\int_1^{\infty}\frac{\phi_1(x)}{f_1(x)}\text{d}x=\int_1^{\infty}\frac{\phi_2(x)}{f_2(x)}\text{d}x,
\end{align*}
since $\int_1^{\infty}\phi_1(x)\text{d}x=\int_1^{\infty}\phi_2(x)\text{d}x=1$, we can do importance sampling as below by setting $\phi_1(x), \phi_2(x)$ as the density function. For $\phi_1(x)$, we generate the random variates by inverse transformation,
\begin{align*}
F_1(x)=1-e^{\frac{1}{2}(1-x^2)}, F_1^{-1}(u)=\sqrt{1-2\log(1-u)}.
\end{align*}
For $\phi_2(x)$, we generate random variates using acceptance-rejection algorithm.

```{r}
set.seed(123)
m <- 1000

# phi_1 inverse transformation
f1 <- function(x){
  sqrt(2*pi*exp(1))/x
}
IF1 <- function(u){
  sqrt(1-2*log(1-u))
}
u <- runif(m)
x <- IF1(u)
estimate1 <- mean(1/f1(x))
var1 <- var(1/f1(x))

# phi_2 acceptance-rejection algorithm
f2 <- function(x){
  1/x^2/(1-pnorm(1))
}
j <- k <- 0
y <- numeric(m)
while(k<m){
  u <- runif(1)
  j <- j + 1
  x <- rnorm(1) #random variate from g(.)
  if (ifelse(x>1,1,0)>u) {
    #accept x
    k <- k + 1
    y[k] <- x
  }
}
estimate2 <- mean(1/f2(y))
var2 <- var(1/f2(y))

# theoretical integral
g <- function(x){
  x^2*exp(-x^2/2)/sqrt(2*pi)
}
value <- integrate(g,1,Inf)$value
cat(value, estimate1, estimate2)
cat(var1, var2)
```

We can find that importance function $f_1$ produce the smaller variance in estimating the integral.

### 5.15
#### Question
Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.

#### Answer
We need to estimate the integral $\theta=\int_0^1\frac{e^{-x}}{1+x^2}\text{d}x$. Notice that all the functions mentioned are supported on $(0,1)$. From Example 5.10 we know that the perfect importance function is $f(x)=\frac{e^{-x}}{1-e^{-1}}$. So we set the stratified importance functions as
\begin{align*}
f_j(x)=\frac{e^{-x}}{e^{-a_{j-1}}-e^{-a_j}}, a_{j-1}<x<a_j, j=1,\dots,5,
\end{align*}
where $a_j=j/5, j=0,\dots,5$. We generate variates from $f_j(x)$ by inverse transformation.
\begin{align*}
F_j(x)=\frac{e^{-a_{j-1}}-e^{-x}}{e^{-a_{j-1}}-e^{-a_j}}, 
F_j^{-1}(u)=-\log(e^{-a_{j-1}}-(e^{-a_{j-1}}-e^{-a_j})u), 
a_{j-1}<x<a_j, j=1,\dots,5.
\end{align*}
Then
\begin{align*}
\theta=\sum_{j=1}^{5}\theta_j=\sum_{j=1}^{5}\int_{a_{j-1}}^{a_j}\frac{g(x)}{f_j(x)}f_j(x)\text{d}x,
\end{align*}
the estimate $\hat{\theta}=\sum_{j=1}^{5}\hat{\theta_j}=\sum_{j=1}^{5}\frac{1}{m}\frac{g(X_j)}{f_j(X_j)}, X_j\sim f_j(x)$.

```{r}
# stratified importance sampling estimate
M <- 20
k <- 5
m <- M/k
set.seed(123)
f <- list()
IF <- list()
gf <- list()
a <- seq(0,1,by=1/k)
g <- function(x){
  exp(-x)/(1+x^2)
}
estimate <- numeric(k)
var <- numeric(k)
for(j in 1:5){
  f[[j]] <- function(x){
    exp(-x)/(exp(-a[j])-exp(-a[j+1]))
  }
  IF[[j]] <- function(u){
    -log(exp(-a[j])-(exp(-a[j])-exp(-a[j+1]))*u)
  }
  gf[[j]] <- function(x){
    (exp(-a[j])-exp(-a[j+1]))/(1+x^2)
  }
  u <- runif(m)
  estimate[j] <- mean(gf[[j]](IF[[j]](u)))
  var[j] <- var(gf[[j]](IF[[j]](u)))
}
cat(integrate(g,0,1)$value, sum(estimate), sum(var))
cat((0.09658794^2-8.817495e-05)/0.09658794^2)
```

Compared to Example 5.10 we can see that the estimate variance has reduced over 99%.

### 6.5
#### Question
Suppose a 95% symmetric t-interval is applied to estimate a mean, but the
sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment to estimate the coverage probability of the t-interval for random samples of $\chi^2(2)$ data with sample size n = 20. Compare your t-interval results with the simulation results in Example 6.4. (The t-interval should be more robust to departures from normality than the interval for variance.)


#### Answer
We need to construct a confidence interval for mean of $\chi^2(2)$ using t-distribution, which is $2$ theoretically, and calculate the MC coverage probability. For each sample, $\frac{\bar{X}}{S/\sqrt{n}}$ asymptotically follows t distribution with degree $n-1$. Thus the confidence interval is
\begin{align*}
[\bar{X}+t_{n-1}(\alpha/2)S/\sqrt{n},\bar{X}-t_{n-1}(\alpha/2)S/\sqrt{n}].
\end{align*}

```{r}
set.seed(123)
m <- 1000
n <- 20
alpha <- 0.05
sample <- replicate(m, expr = {
x <- rchisq(n,2)
} )
c1 <- apply(sample,2,mean)+qt(alpha/2,n-1)*apply(sample,2,sd)/sqrt(n)
c2 <- apply(sample,2,mean)-qt(alpha/2,n-1)*apply(sample,2,sd)/sqrt(n)
mean(ifelse(c1<2 & c2>2,1,0))              
```

### 6.A
#### Question
Use Monte Carlo simulation to investigate whether the empirical Type I error
rate of the t-test is approximately equal to the nominal significance level $\alpha$, when the sampled population is non-normal. The t-test is robust to mild departures from normality. Discuss the simulation results for the cases where the sampled population is (i) $\chi^2(1)$, (ii) Uniform$(0,2)$, and (iii) Exponential$(\text{rate} = 1)$. In each case, test $H_0 : \mu = \mu_0$ vs $H_1 : \mu \ne \mu_0$, where $\mu_0$ is the mean of $\chi^2(1)$, Uniform$(0,2)$, and Exponential$(1)$, respectively.

#### Answer
```{r}
set.seed(123)
m <- 1000
n <- 20
alpha <- 0.05
sample1 <- replicate(m, expr = {
  x <- rchisq(n,1)
} )
sample2 <- replicate(m, expr = {
  x <- runif(n,0,2)
} )
sample3 <- replicate(m, expr = {
  x <- rexp(n)
} )

test <- function(sample,mu){
  c1 <- mu+qt(alpha/2,n-1)*apply(sample,2,sd)/sqrt(n)
  c2 <- mu+qt(1-alpha/2,n-1)*apply(sample,2,sd)/sqrt(n)
  xbar <- apply(sample,2,mean)
  mean(ifelse(xbar<c1|xbar>c2,1,0))
}
test(sample1,1)
test(sample2,1)
test(sample3,1)
```

## Homework 5 (2023-10-16)

### 1
#### Question
Suppose $m=1000$ tests. Null hypothesis is not rejected in the the first $95\%$ tests while it is rejected at the last $5\%$. The distribution of p-value is $U(0,1)$ under null hypothesis and $B(0.1,1)$ under alternative hypothesis. Modify p-values by applying Bonferroni correction and B-H correction and compare them with the nominal level $\alpha=0.1$ and decide whether rejecting null hypothesis or not. Based on $M=1000$ simulations, try to estimate FWER, FDR, TPR and list them in the table.

#### Answer

```{r}
library(foreach)
hypothesis <- function(method){
set.seed(10)
estimate <- foreach(i = 1:1000, .combine = "rbind") %do% {
  p0 <- runif(950)
  p1 <- rbeta(50,0.1,1)
  p <- c(p0,p1)
  p0.adjust <- p.adjust(p, method = method)[1:950]
  p1.adjust <- p.adjust(p, method = method)[951:1000]
  V <- sum(p0.adjust<0.1)
  S <- sum(p1.adjust<0.1)
  R <- V+S
  result <- c(V>=1, V/R, S/50)
  names(result) <- c('FWER','FDR','TPR')
  result
}
apply(estimate,2,mean)
}
hypothesis('bonferroni')
hypothesis('BH')
```

method|FWER|FDR|TPR
:-:|:-:|:-:|:-:
bonferroni|0.094000000 |0.004736298 |0.400220000 
B-H|0.93900000 |0.09660221 |0.56392000

### 2
#### Question
Suppose the population has the exponential distribution with rate $\lambda$, then the MLE of $\lambda$ is $\hat{\lambda}=1/\bar{X}$, where $\bar{X}$ is the sample mean. It can be derived that the expectation of $\hat{\lambda}$ is $\lambda n/(n-1)$, so that the estimation bias is $\lambda/(n-1)$. The standard error of $\hat{\lambda}$ is $\lambda n/[(n-1)\sqrt{n-2}]$. Conduct a simulation study to verify the performance of the bootstrap method.

- The true value $\lambda=2$.
- The sample size $n=5,10,20$.
- The number of bootstrap replicates $B=1000$.
- The simulations are repeated for $m=1000$ times.
- Compare the mean bootstrap bias and bootstrap standard error with the theoretical ones. Comment on the results.

#### Answer
```{r}
result <- function(n){
set.seed(123)
m <- 100
B <- 100
simulation <- foreach(i=1:m, .combine = 'rbind') %do% {
  x <- rexp(n,rate=2)
  lambda.hat <- 1/mean(x)
  bootstrap <- foreach(i=1:B, .combine = 'c') %do% {
    b <- sample(1:n, size = n, replace = TRUE)
    1/mean(x[b])
  }
  bias.bootstrap <- mean(bootstrap)-lambda.hat
  standard.error.bootstrap <- sd(bootstrap)
  temp <- c(bias.bootstrap,standard.error.bootstrap)
  names(temp) <- c('bias.bootstrap','standard.error.bootstrap')
  temp
}
cat('theoretical results:',2/(n-1),2*n/(n-1)/sqrt(n-2),'\n')
apply(simulation, 2, mean)
}

result(5)
result(10)
result(20)

```

We can find that the bootstrap estimate of bias and standard error are very close to the theoretical values and it get closer when the sample size rises.


### 7.3
#### Question
Obtain a bootstrap t confidence interval estimate for the correlation statistic in Example 7.2 (law data in bootstrap).

#### Answer
```{r}
library(bootstrap)
set.seed(123)
B <- 200
n <- nrow(law)
alpha <- 0.05
R.hat <- cor(law$LSAT,law$GPA)
bootstrap <- foreach(i = 1:B, .combine = 'c') %do% {
  b <- sample(1:n, size = n, replace = TRUE)
  LSAT <- law$LSAT[b]
  GPA <- law$GPA[b]
  R.hat.b <- cor(LSAT, GPA)
  R.hat.b.k <- foreach(j = 1:B, .combine = 'c') %do% {
    k <- sample(1:n, size = n, replace = TRUE)
    LSAT <- law$LSAT[k]
    GPA <- law$GPA[k]
    cor(LSAT, GPA)
  }  
  t.b <- (R.hat.b-R.hat)/sd(R.hat.b.k)
}
Qt <- quantile(bootstrap, c(alpha/2, 1-alpha/2), type = 1)
R.hat+Qt*sd(bootstrap)
R.hat
```

A bootstrap t confidence interval is $[-1.438737, 2.216742]$.

## Homework 6 (2023-10-23)

### 7.5
#### Question
Refer to Exercise 7.4. Compute $95\%$ bootstrap confidence intervals for the mean time between failures $1/\lambda$ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

> Exercise 7.4 
> 
> Refer to the air-conditioning data set `aircondit` provided in the `boot` package. The 12 observations are the times in hours between failures of air-conditioning equipment [63, Example 1.1]:
> 
> 3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487.
> 
> Assume that the times between failures follow an exponential model $Exp(\lambda)$.


#### Answer
```{r warning=FALSE}
library(boot)
stat.1 <- function(data, index) { # index is the sample after resampling
  mean(data[index])
}
stat.2 <- function(data, index) { # index is the sample after resampling
  1/mean(data[index])
}
set.seed(123)
boot.1 <- boot(data = aircondit[,1], statistic = stat.1, R = 1e3)
set.seed(123)
boot.2 <- boot(data = aircondit[,1], statistic = stat.2, R = 1e3)
ci.1 <- boot.ci(boot.1, type = c("norm","basic","perc","bca"))
ci.2 <- boot.ci(boot.2, type = c("norm","basic","perc","bca"))
print(ci.1) # 1/lambda
print(ci.2) # lambda

compare <- cbind(
  rbind(ci.1$normal[2:3],ci.1$basic[4:5],ci.1$percent[4:5],ci.1$bca[4:5]),
  rbind(1/ci.2$normal[2:3],1/ci.2$basic[4:5],1/ci.2$percent[5:4],1/ci.2$bca[5:4]))
rownames(compare) <- c("norm","basic","perc","bca")
colnames(compare) <- c('ci.1.left','ci.1.right','1/ci.2.left','1/ci.2.right')
print(compare)
```

We can see that the percentile confidence interval and the BCa confidence interval are transformation respecting.

### 7.8
#### Question
Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat{\theta}$.

> Exercise 7.7
>
> Refer to Exercise 7.6. Efron and Tibshirani discuss the following example [84,Ch. 7]. The five-dimensional scores data have a $5 \times 5$ covariance matrix $\Sigma$, with positive eigenvalues $\lambda_1 > \dots > \lambda_5$. In principal components analysis,
> $$\theta=\frac{\lambda_1}{\sum_{j=1}^{5}\lambda_j}$$
> measures the proportion of variance explained by the first principal component. Let $\hat{\lambda}_1 > \dots > \hat{\lambda}_5$ be the eigenvalues of $\hat{\Sigma}$, where $\hat{\Sigma}$ is the MLE of $\Sigma$. Compute the sample estimate
> $$\hat{\theta}=\frac{\hat{\lambda}_1}{\sum_{j=1}^{5}\hat{\lambda}_j}$$
> of $\theta$.
> 
> > Exercise 7.6
> > 
> > Efron and Tibshirani discuss the `scor` (`bootstrap`) test score data on 88 students who took examinations in five subjects [84, Table 7.1], [188, Table 1.2.1]. The first two tests (mechanics, vectors) were closed book and the last three tests (algebra, analysis, statistics) were open book. Each row of the data frame is a set of scores $(x_{i1},\dots,x_{i5})$ for the $i^{\text{th}}$ student.

#### Answer

```{r warning=FALSE}
library(bootstrap)
data <- as.matrix(scor)
n <- nrow(data)

stat <- function(data) {
  sigma.hat <-  var(data)
  lambda <- eigen(sigma.hat)$values
  theta.hat <- sort(lambda)[1]/sum(lambda)
  return(theta.hat)
}

theta.hat <- stat(data)
theta.jack <- numeric(n)
for (i in 1:n)
theta.jack[i] <- stat(data[-i,])
bias <- (n - 1) * (mean(theta.jack) - theta.hat)
se <- sqrt((n-1) * mean((theta.jack - mean(theta.jack))^2))
cat('bias: ',bias,'\n','se: ',se)
```

### 7.11
#### Question
In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Use leave-two-out cross validation to compare the models.

> Example 7.18
> 
> Cross validation is applied to select a model in Example 7.17.
> 
> > Example 7.17
> > 
> > The `ironslag` (`DAAG`) data [185] has 53 measurements of iron content by two methods, chemical and magnetic (see “iron.dat” in [126]).

#### Answer
```{r warning=FALSE}
library(DAAG)
attach(ironslag)
n <- length(magnetic) #in DAAG ironslag
e1 <- e2 <- e3 <- e4 <- numeric(n)
# for n-fold cross validation
# fit models on leave-two-out samples
for (index in 1:choose(n,2)) {
k <- combn(n,2)[,index]
y <- magnetic[-k]
x <- chemical[-k]
J1 <- lm(y ~ x)
yhat1 <- J1$coef[1] + J1$coef[2] * chemical[k]
e1[k] <- magnetic[k] - yhat1
J2 <- lm(y ~ x + I(x^2))
yhat2 <- J2$coef[1] + J2$coef[2] * chemical[k] +
J2$coef[3] * chemical[k]^2
e2[k] <- magnetic[k] - yhat2
J3 <- lm(log(y) ~ x)
logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[k]
yhat3 <- exp(logyhat3)
e3[k] <- magnetic[k] - yhat3
J4 <- lm(log(y) ~ log(x))
logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[k])
yhat4 <- exp(logyhat4)
e4[k] <- magnetic[k] - yhat4
}
print(c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2)))
```

According to the prediction error criterion, Model 2, the quadratic model, would be the best fit for the data.

## Homework 7 (2023-10-30)

### 1
#### Question
Prove the stationarity of Metropolis-Hastings Algorithm in continuous situation.

#### Answer
Suppose that the target distribution has density function $f$ and the proposal distribution is $Q$. For each $x$, $Q(x,\cdot)$ has the transition kernel density $q(\cdot|x)$. After taking a sample $y$ from $Q(x,\cdot)$, we accept it by probability
\begin{align*}
\alpha(x,y)=\min\{1,\frac{f(y)q(x|y)}{f(x)q(y|x)}\}.
\end{align*}
So the actual transition kernel is
\begin{align*}
p(x,y)=q(y|x)\alpha(x,y)=q(y|x)\min\{1,\frac{f(y)q(x|y)}{f(x)q(y|x)}\}.
\end{align*}
Thus
\begin{align*}
p(x,y)f(x)&=f(x)q(y|x)\min\{1,\frac{f(y)q(x|y)}{f(x)q(y|x)}\}\\
&=f(x)q(y|x)\frac{f(y)q(x|y)}{f(x)q(y|x)}\frac{f(x)q(y|x)}{f(y)q(x|y)}\min\{1,\frac{f(y)q(x|y)}{f(x)q(y|x)}\}\\
&=f(y)q(x|y)\min\{1,\frac{f(x)q(y|x)}{f(y)q(x|y)}\}\\
&=p(y,x)f(y).
\end{align*}
Then $f$ satisfies the detailed balance condition $p(x,y)f(x)=p(y,x)f(y)$. Therefore $f$ is the stationary distribution of the chain.

### 8.1
#### Question
Implement the two-sample Cramér-von Mises test for equal distributions as a permutation test. Apply the test to the data in Examples 8.1 and 8.2.

#### Answer

```{r}
library(twosamples)
pcvm <- function(x,y){
R <- 999 #number of replicates
n <- length(x)
m <- length(y)
z <- c(x, y) #pooled sample
K <- 1:(n+m)
D <- numeric(R) #storage for replicates
D0 <- cvm_test(x, y)[1]
for (i in 1:R) {
#generate indices k for the first sample
k <- sample(K, size = n, replace = FALSE)
x1 <- z[k]
y1 <- z[-k] #complement of x1
D[i] <- cvm_test(x1, y1)[1]
}
p <- mean(c(D0, D) >= D0)
return(p)
}

# Example 8.1 & 8.2
attach(chickwts)
x <- sort(as.vector(weight[feed == "soybean"]))
y <- sort(as.vector(weight[feed == "linseed"]))
pcvm(x,y)
detach(chickwts)
```


### 8.3
#### Question
The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion is not applicable for unequal sample sizes. Implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal.

#### Answer

```{r}
set.seed(10)
max.num.extreme <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
return(max(c(outx, outy)))
}

R <- 999 #number of replicates
n1 <- 20
n2 <- 30
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
z <- c(x, y) #pooled sample
K <- 1:(n1+n2)
D <- numeric(R) #storage for replicates
D0 <- max.num.extreme(x, y)
for (i in 1:R) {
#generate indices k for the first sample
k <- sample(K, size = n1, replace = FALSE)
x1 <- z[k]
y1 <- z[-k] #complement of x1
D[i] <- max.num.extreme(x1,y1)
}
p <- mean(c(D0, D) >= D0)
print(p)
```

The p-value is over 0.05, so we accept $H_0$. The variances are equal.

## Homework 8 (2023-11-06)

### 1
#### Question
Consider a model 
\begin{align*}
P\left(Y=1 \mid X_{1}=x_1, X_{2}=x_2, X_{3}=x_3\right)=\frac{\exp \left(a+b_{1} x_{1}+b_{2} x_{2}+b_{3} x_{3}\right)}{1+\exp \left(a+b_{1} x_{1}+b_{2} x_{2}+b_{3} x_{3}\right)},
\end{align*}
where $X_{1} \sim P(1)$, $X_{2} \sim \operatorname{Exp}(1)$  and $X_{3} \sim B(1,0.5)$.

- Design a function that takes $N, b_{1}, b_{2}, b_{3}$ and $f_{0}$ as input values, and produces the output $a$.

- Call this function, input values are $N=10^{6}, b_{1}=0, b_{2}=1, b_{3}=-1, f_{0}=0.1,0.01,0.001,0.0001$.

- Plot $-\log f_{0}$ vs $a$.

#### Answer
```{r}
set.seed(123)
LogisticIntercept <- function(N,b1,b2,b3,f0){
  x1 <- rpois(N,lambda = 1)
  x2 <- rexp(N, rate = 1)
  x3 <- sample(0:1,N,replace=TRUE)
  g <- function(alpha){
    tmp <- exp(-alpha-b1*x1-b2*x2-b3*x3); p <- 1/(1+tmp)
    mean(p) - f0
  }
  solution <- uniroot(g,c(-100,100))
  #round(unlist(solution),5)[1:3]
  alpha <- solution$root
  return(alpha)
}
N <- 1e6
b1 <- 0
b2 <- 1
b3 <- -1
f0 <- c(0.1,0.01,0.001,0.0001)

alpha <- numeric(4)
for(i in 1:4)
alpha[i] <- LogisticIntercept(N,b1,b2,b3,f0[i])
names(alpha) <- c('f0=0.1','f0=0.01','f0=0.001','f0=0.0001')
print(alpha)

plot(-log(f0),alpha)
```


### 9.4
#### Question
Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.

#### Answer
```{r}
f <- function(x){
  exp(-abs(x))/2
}

rw.Metropolis <- function(sigma, x0, N) {
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
  y <- rnorm(1, x[i-1], sigma)
  if (u[i] <= (f(y) / f(x[i-1])))
    x[i] <- y
  else {
    x[i] <- x[i-1]
    k <- k + 1
  }
}
return(list(x=x, k=k))
}

N <- 2000
x0 <- 10
sigma <- c(.05, .5, 2, 16)
res <- list()
# par(mfrow = c(2, 2))
acceptance.rate <- numeric(4)
for(i in 1:4){
  res[[i]] <- rw.Metropolis(sigma[i],x0,N)
  # plot(1:N,res[[i]]$x,xlab = '',ylab = 'points',type = 'l')
  acceptance.rate[i] <- (N-res[[i]]$k)/N
}
names(acceptance.rate) <- c('sigma=0.05','sigma=0.5','sigma=2','sigma=16')
print(acceptance.rate)
```


### 9.7
#### Question
Implement a Gibbs sampler to generate a bivariate normal chain $(X_t, Y_t)$ with zero means, unit standard deviations, and correlation $0.9$. Plot the generated sample after discarding a suitable burn-in sample. Fit a simple linear regression model $Y = \beta_0 + \beta_1X$ to the sample and check the residuals of the model for normality and constant variance.

#### Answer
```{r}
#initialize constants and parameters
set.seed(123)
N <- 5000 #length of chain
burn <- 1000 #burn-in length
X <- matrix(0, N, 2) #the chain, a bivariate sample
rho <- 0.9 #correlation
mu1 <- 0
mu2 <- 0
sigma1 <- 1
sigma2 <- 1
s1 <- sqrt(1-rho^2)*sigma1
s2 <- sqrt(1-rho^2)*sigma2
###### generate the chain #####
X[1, ] <- c(mu1, mu2) #initialize
for (i in 2:N) {
x2 <- X[i-1, 2]
m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
X[i, 1] <- rnorm(1, m1, s1)
x1 <- X[i, 1]
m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
X[i, 2] <- rnorm(1, m2, s2)
}
b <- burn + 1
x <- X[b:N, ]

plot(x[,1],x[,2])
fit <- lm(x[,2]~1+x[,1])
summary(fit)
```


### 9.10
#### Question
Refer to Example 9.1. Use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until the chain has converged approximately to the target distribution according to $\hat{R} < 1.2$. (See Exercise 9.9.) Also use the `coda` [212] package to check for convergence of the chain by the Gelman-Rubin method. Hints: See the help topics for the `coda` functions `gelman.diag`, `gelman.plot`, `as.mcmc`, and `mcmc.list`.

#### Answer
```{r}
library(coda)
set.seed(123)

f <- function(x, sigma) {
if (any(x < 0)) return (0)
stopifnot(sigma > 0)
return((x / sigma^2) * exp(-x^2 / (2*sigma^2)))
}

Rayleigh.chain <- function(m,sigma){
x <- numeric(m)
x[1] <- rchisq(1, df=1)
k <- 0
u <- runif(m)
for (i in 2:m) {
xt <- x[i-1]
y <- rchisq(1, df = xt)
num <- f(y, sigma) * dchisq(xt, df = y)
den <- f(xt, sigma) * dchisq(y, df = xt)
if (u[i] <= num/den) x[i] <- y 
else {
x[i] <- xt
k <- k+1 #y is rejected
}
}
return(x)
}

Gelman.Rubin <- function(psi) {
# psi[i,j] is the statistic psi(X[i,1:j])
# for chain in i-th row of X
psi <- as.matrix(psi)
n <- ncol(psi)
k <- nrow(psi)
psi.means <- rowMeans(psi) #row means
B <- n * var(psi.means) #between variance est.
psi.w <- apply(psi, 1, "var") #within variances
W <- mean(psi.w) #within est.
v.hat <- W*(n-1)/n + (B/n) #upper variance est.
r.hat <- v.hat / W #G-R statistic
return(r.hat)
}

#generate the chains
m <- 15000 #length of chains
sigma <- 4
k <- 1
mcmc <- mcmc.list()
mcmc[[k]] <- as.mcmc(Rayleigh.chain(m,sigma))
X <- Rayleigh.chain(m,sigma)
rhat <- 10
while(isTRUE(rhat[k] >= 1.2)){
  X <- rbind(X,Rayleigh.chain(m,sigma))
  psi <- t(apply(X, 1, cumsum))
  for (i in 1:nrow(psi))
    psi[i,] <- psi[i,] / (1:ncol(psi))
  rhat <- c(rhat,Gelman.Rubin(psi))
  k <- k+1
  mcmc[[k]] <- as.mcmc(Rayleigh.chain(m,sigma))
}
print(k)
gelman.diag(mcmc)
gelman.plot(mcmc)
```

## Homework 9 (2023-11-13)

### Extra Exercise
```{r}
u <- c(11,8,27,13,16,0,23,10,24,2)
v <- u + 1
n <- length(u)

f <- function(lambda){
  sum((u*exp(-lambda*u)-v*exp(-lambda*v))/(exp(-lambda*u)-exp(-lambda*v)))
}

# MLE
uniroot(f,c(0,1))$root

# EM
lambda.hat <- 1/mean(u)
for(k in 1:1e4){
  g <- function(lambda){
    temp <- f(lambda.hat[k])+1/lambda.hat[k]
    n/lambda-temp+f(lambda)
  }
  lambda.hat <- c(lambda.hat, uniroot(g(lambda.hat[k]),c(0,1))$root)
  if(abs(lambda.hat[k+1]-lambda.hat[k])<1e-7) break
}
lambda.hat[k]
```


### 11.8

#### Question
In the Morra game, the set of optimal strategies are not changed if a constant is subtracted from every entry of the payoff matrix, or a positive constant is multiplied times every entry of the payoff matrix. However, the simplex algorithm may terminate at a different basic feasible point (also optimal). Compute B <- A + 2, find the solution of game B, and verify that it is one
of the extreme points (11.12)–(11.15) of the original game A. Also find the value of game A and game B.

#### Answer

```{r}
solve.game <- function(A) {
#solve the two player zero-sum game by simplex method
#optimize for player 1, then player 2
#maximize v subject to ...
#let x strategies 1:m, and put v as extra variable
#A1, the <= constraints
#
min.A <- min(A)
A <- A - min.A #so that v >= 0
max.A <- max(A)
A <- A / max(A)
m <- nrow(A)
n <- ncol(A)
it <- n^3
a <- c(rep(0, m), 1) #objective function
A1 <- -cbind(t(A), rep(-1, n)) #constraints <=
b1 <- rep(0, n)
A3 <- t(as.matrix(c(rep(1, m), 0))) #constraints sum(x)=1
b3 <- 1
sx <- simplex(a=a, A1=A1, b1=b1, A3=A3, b3=b3,
maxi=TRUE, n.iter=it)
#the ’solution’ is [x1,x2,...,xm | value of game]
#
#minimize v subject to ...
#let y strategies 1:n, with v as extra variable
a <- c(rep(0, n), 1) #objective function
A1 <- cbind(A, rep(-1, m)) #constraints <=
b1 <- rep(0, m)
A3 <- t(as.matrix(c(rep(1, n), 0))) #constraints sum(y)=1
b3 <- 1
sy <- simplex(a=a, A1=A1, b1=b1, A3=A3, b3=b3,
maxi=FALSE, n.iter=it)
soln <- list("A" = A * max.A + min.A,
"x" = sx$soln[1:m],
"y" = sy$soln[1:n],
"v" = sx$soln[m+1] * max.A + min.A)
soln
}

#enter the payoff matrix
A <- matrix(c( 0,-2,-2,3,0,0,4,0,0,
2,0,0,0,-3,-3,4,0,0,
2,0,0,3,0,0,0,-4,-4,
-3,0,-3,0,4,0,0,5,0,
0,3,0,-4,0,-4,0,5,0,
0,3,0,0,4,0,-5,0,-5,
-4,-4,0,0,0,5,0,0,6,
0,0,4,-5,-5,0,0,0,6,
0,0,4,0,0,5,-6,-6,0), 9, 9)
library(boot) #needed for simplex function
s.A <- solve.game(A)
round(cbind(s.A$x, s.A$y), 7)
B <- A+2
s.B <- solve.game(B)
round(cbind(s.B$x, s.B$y), 7)
value.A <- t(s.A$x)%*%(A)%*%(s.A$y)
value.B <- t(s.B$x)%*%(A+2)%*%(s.B$y)
round(c(value.A,value.B),3)
```

## Homework 10 (2023-11-20)

### 2.1.3 Exercise 4
#### Question
Why do you need to use `unlist()` to convert a list to an atomic vector? Why doesn’t `as.vector()` work?

#### Answer
Because if `mode = "any"`, `is.vector` may return `TRUE` for the atomic modes, list and expression, then `as.vector()` doesn't work.

### 2.3.1 Exercise 1, 2
#### Question
1. What does `dim()` return when applied to a vector?  
2. If `is.matrix(x)` is TRUE, what will `is.array(x)` return?

#### Answer
1. NULL
2. TRUE

### 2.4.5 Exercise 2, 3
#### Question
2. What does `as.matrix()` do when applied to a data frame with columns of different types?  
3. Can you have a data frame with 0 rows? What about 0 columns?

#### Answer
2. They will be coerced to the most flexible type. Types from least to most flexible are: logical, integer, double, and character. If the type of one column is list, it will be unlisted only when the length of each elements in the list is the same.  
3. Yes, just use `data.frame()`.

### Exercises 2
#### Question
The function below scales a vector so it falls in the range [0,1]. How would you apply it to every column of a data frame? How would you apply it to every numeric column in a data frame?
```
scale01 <- function(x) {
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
}
```
#### Answer
```{r}
scale01 <- function(x) {
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
}
library(dplyr)
df <- data.frame(x=1:3,y=c('a','b','c'),z=c(4,6,10))
lapply(df %>% select_if(is.numeric),scale01)
```

### Exercises 1
#### Question
Use `vapply()` to:  
a) Compute the standard deviation of every column in a numeric data frame.  
b) Compute the standard deviation of every numeric column in a mixed data frame. (Hint: you’ll need to use `vapply()` twice.)  

#### Answer
```{r}
df <- data.frame(x=1:3,y=c(4,6,10))
vapply(df,sd,numeric(1))
df <- data.frame(x=1:3,y=c('a','b','c'),z=c(4,6,10))
vapply(df %>% select_if(is.numeric),sd,numeric(1))
```

### Exercise 9.8
#### Question
Consider the bivariate density
\begin{align*}
f(x, y) \propto\left(\begin{array}{l}
n \\
x
\end{array}\right) y^{x+a-1}(1-y)^{n-x+b-1}, \quad x=0,1, \ldots, n, \quad 0 \leq y \leq 1.
\end{align*}
It can be shown (see e.g. [23]) that for fixed a, b, n, the conditional distributions are Binomial(n, y) and Beta(x+a, n−x+b). Use the Gibbs sampler to generate a chain with target joint density f(x, y).
